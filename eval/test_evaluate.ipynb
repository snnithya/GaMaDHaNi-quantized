{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import gin\n",
    "from functools import partial\n",
    "import copy\n",
    "from gamadhani.utils.utils import download_models, download_data\n",
    "from gamadhani.utils.generate_utils import load_audio_fns\n",
    "from gamadhani.utils.utils import get_device\n",
    "import gamadhani.utils.pitch_to_audio_utils as p2a\n",
    "\n",
    "\n",
    "# Load Model\n",
    "_, _, audio_path, audio_qt = download_models('kmaneeshad/GaMaDHaNi', pitch_model_type=\"diffusion\")\n",
    "base_model, audio_qt, audio_seq_len, invert_audio_fn = load_audio_fns(audio_path=audio_path, qt_path=audio_qt, config_path='../configs/pitch_to_audio_config.gin')\n",
    "\n",
    "# Parse pitch config\n",
    "gin.parse_config_file('../configs/diffusion_pitch_config.gin')\n",
    "Task_ = gin.get_configurable('src.dataset.Task')\n",
    "task_obj = Task_()\n",
    "pitch_task_fn = partial(task_obj.read_)\n",
    "invert_pitch_task_fn = partial(task_obj.invert_)\n",
    "\n",
    "\n",
    "# Prepare Test Set\n",
    "test_set = np.load(download_data('kmaneeshad/GaMaDHaNi-db'), allow_pickle=True)['concatenated_array']\n",
    "n_test = test_set.shape[0]\n",
    "test_pitches = np.array([test_set[i,0][:2400] for i in range(n_test)])\n",
    "test_audios = np.array([test_set[i,1] for i in range(n_test)])\n",
    "test_audios_resampled_truncated = torch.stack([torchaudio.functional.resample(torch.tensor(test_audios[i]), orig_freq=44100, new_freq=16000) for i in range(n_test)])[...,:191744]\n",
    "test_singer_ids = np.array([test_set[i,4] for i in range(n_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f0_input(pitch_val):\n",
    "    processed_pitch_val = pitch_task_fn(**{\"inputs\": {\"pitch\": {\"data\": pitch_val}}})['sampled_sequence']\n",
    "    processed_pitch_val = torch.Tensor(processed_pitch_val).reshape(1, 1, processed_pitch_val.shape[0])\n",
    "    interpolated_pitch = p2a.interpolate_pitch(pitch=processed_pitch_val, audio_seq_len=audio_seq_len)    # interpolate pitch values\n",
    "    interpolated_pitch = torch.nan_to_num(interpolated_pitch, nan=196)\n",
    "    interpolated_pitch = interpolated_pitch.squeeze(1)\n",
    "    f0 = interpolated_pitch.float()\n",
    "    return f0\n",
    "\n",
    "def generate_audio(model, pitch_val, singer_id, num_steps=100, audio_only=True):\n",
    "    model = model.cuda()\n",
    "    f0 = get_f0_input(pitch_val).to(model.device)\n",
    "    singer_tensor = torch.tensor(np.repeat([singer_id], repeats=f0.shape[0])).to(model.device)\n",
    "\n",
    "    if audio_only:\n",
    "        out_spec, _, _, _ =  model.sample_cfg(f0.shape[0], f0=f0, num_steps=num_steps, singer=singer_tensor, strength=3, invert_audio_fn=invert_audio_fn)\n",
    "        audio = invert_audio_fn(out_spec)\n",
    "        return audio\n",
    "    else:\n",
    "        out_spec, out_pitch, singer_id, all_activations = model.sample_cfg(f0.shape[0], f0=f0, num_steps=num_steps, singer=singer_tensor, strength=3, invert_audio_fn=invert_audio_fn, log_interim_samples=True, log_interim_forward_activations=True)\n",
    "        audio = invert_audio_fn(out_spec)\n",
    "        return out_spec, out_pitch, singer_id, all_activations, audio\n",
    "  \n",
    "def generate_all_audios(model, num_steps):\n",
    "  generated_audios = []\n",
    "  for i in range(n_test):\n",
    "     generated_audios.append(generate_audio(model, test_pitches[i], test_singer_ids[i], num_steps=num_steps))\n",
    "  return torch.cat(generated_audios, dim=0)\n",
    "    \n",
    "\n",
    "def quantize_per_tensor(x, bits=4, min_val=None, max_val=None):\n",
    "    if min_val is None:\n",
    "        min_val = x.min()\n",
    "    if max_val is None:\n",
    "        max_val = x.max()\n",
    "    targets = torch.linspace(min_val, max_val, 2**bits).to(x.device)\n",
    "    differences = torch.abs(x.unsqueeze(-1) - targets)\n",
    "    nearest_indices = torch.argmin(differences, dim=-1)\n",
    "    rounded_values = targets[nearest_indices]\n",
    "    return rounded_values\n",
    "\n",
    "def quantize_model(net, bits, mode='global'):\n",
    "  quantized_net = copy.deepcopy(net)\n",
    "  for name, module in quantized_net.named_modules():\n",
    "    if isinstance(module, (torch.nn.Conv1d, torch.nn.Linear, torch.nn.ConvTranspose1d)):\n",
    "      if mode == 'global':\n",
    "        module.weight.data = quantize_per_tensor(module.weight.data, bits=bits)\n",
    "      elif mode == 'per_channel':\n",
    "        for i in range(module.weight.data.shape[0]):\n",
    "          module.weight.data[i] = quantize_per_tensor(module.weight.data[i], bits=bits)\n",
    "\n",
    "  if hasattr(module, 'bias') and module.bias is not None and mode == 'per_channel':\n",
    "      module.bias.data = quantize_per_tensor(module.bias.data, bits=bits)\n",
    "  return quantized_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 1495.63it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1447.17it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1444.26it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1500.56it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1486.76it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1549.65it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1577.66it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1599.44it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1526.61it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1500.14it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1448.26it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1499.73it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1457.11it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 421.99it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1335.37it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1548.41it/s]\n"
     ]
    }
   ],
   "source": [
    "base_generated = generate_all_audios(base_model, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openl3\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.linalg import sqrtm\n",
    "from pathlib import Path\n",
    "from openl3.models import load_audio_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcd(ref_audio, quant_audio, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Mel Cepstral Distortion (MCD): measures spectral distortion between audio signals.\n",
    "\n",
    "    - Aligns MFCCs (Mel-Frequency Cepstral Coefficients) of reference and quantized audio using DTW.\n",
    "    - Lower MCD = better timbral and pitch preservation.\n",
    "    \"\"\"\n",
    "    ref_wave, _ = librosa.load(ref_audio, sr=sample_rate, mono=True)\n",
    "    quant_wave, _ = librosa.load(quant_audio, sr=sample_rate, mono=True)\n",
    "    ref_mfcc = librosa.feature.mfcc(ref_wave, sr=sample_rate, n_mfcc=13).T\n",
    "    quant_mfcc = librosa.feature.mfcc(quant_wave, sr=sample_rate, n_mfcc=13).T\n",
    "    _, cost, _, path = librosa.sequence.dtw(ref_mfcc, quant_mfcc, metric=\"euclidean\")\n",
    "    return cost / len(path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lsd(ref_audio, quant_audio, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Log Spectral Distance (LSD): measures frequency-domain distortion between audio signals.\n",
    "\n",
    "    - Compares log-scaled power spectra of reference and quantized audio.\n",
    "    - Lower LSD = better preservation of spectral content.\n",
    "    \"\"\"\n",
    "    ref_wave, _ = librosa.load(ref_audio, sr=sample_rate, mono=True)\n",
    "    quant_wave, _ = librosa.load(quant_audio, sr=sample_rate, mono=True)\n",
    "    ref_spectrum = np.abs(librosa.stft(ref_wave)) ** 2\n",
    "    quant_spectrum = np.abs(librosa.stft(quant_wave)) ** 2\n",
    "    log_diff = np.log10(np.maximum(ref_spectrum, 1e-10)) - np.log10(np.maximum(quant_spectrum, 1e-10))\n",
    "    lsd = np.mean(np.sqrt(np.mean(log_diff ** 2, axis=0)))\n",
    "    return lsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_openl3_embeddings(audio_path, model, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    OpenL3 embeddings: Extracts perceptual embeddings from audio using a pre-trained OpenL3 model.\n",
    "    - Embeddings capture timbral and tonal features for perceptual quality evaluation.\n",
    "    \"\"\"\n",
    "    waveform, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    embeddings, _ = openl3.get_audio_embedding(waveform, sr, content_type=\"music\", model=model)\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fad(reference_dir, quantized_dir, model, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Frechet Audio Distance (FAD): measures overall perceptual similarity between audio distributions.\n",
    "\n",
    "    - Compares mean and covariance of OpenL3 embeddings for reference and quantized audio.\n",
    "    - Lower FAD = better overall perceptual similarity.\n",
    "    \"\"\"\n",
    "    ref_embeddings = []\n",
    "    quant_embeddings = []\n",
    "\n",
    "    for ref_audio in Path(reference_dir).iterdir():\n",
    "        if ref_audio.suffix == \".wav\":\n",
    "            ref_embeddings.append(extract_openl3_embeddings(str(ref_audio), model, sample_rate))\n",
    "\n",
    "    for quant_audio in Path(quantized_dir).iterdir():\n",
    "        if quant_audio.suffix == \".wav\":\n",
    "            quant_embeddings.append(extract_openl3_embeddings(str(quant_audio), model, sample_rate))\n",
    "\n",
    "    ref_mu, ref_sigma = np.mean(ref_embeddings, axis=0), np.cov(np.array(ref_embeddings).T)\n",
    "    quant_mu, quant_sigma = np.mean(quant_embeddings, axis=0), np.cov(np.array(quant_embeddings).T)\n",
    "    diff = ref_mu - quant_mu\n",
    "    covmean, _ = sqrtm(ref_sigma @ quant_sigma, disp=False)\n",
    "    covmean = covmean.real if np.isfinite(covmean).all() else np.zeros_like(ref_sigma)\n",
    "    fad_score = np.trace(ref_sigma + quant_sigma - 2 * covmean) + np.dot(diff, diff)\n",
    "    return fad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_audio_quality(ref_dir, quant_dir, model, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Evaluates quantization levels using FAD, MCD, and LSD.\n",
    "    - FAD: measures overall perceptual similarity\n",
    "    - MCD: evaluates timbral and pitch preservation\n",
    "    - LSD: assesses spectral fidelity\n",
    "    \"\"\"\n",
    "    ref_dir = Path(ref_dir)\n",
    "    quant_dir = Path(quant_dir)\n",
    "    results = []\n",
    "\n",
    "    for quantization_level in quant_dir.iterdir():\n",
    "        if quantization_level.is_dir():\n",
    "            print(f\"Evaluating quantization level: {quantization_level.name}\")\n",
    "            fad_score = compute_fad(ref_dir, quantization_level, model, sample_rate)\n",
    "            # print(f\"FAD Score for {quantization_level.name}: {fad_score}\")\n",
    "\n",
    "            for ref_file, quant_file in zip(ref_dir.iterdir(), quantization_level.iterdir()):\n",
    "                if ref_file.suffix == \".wav\" and quant_file.suffix == \".wav\":\n",
    "                    mcd_score = compute_mcd(ref_file, quant_file)\n",
    "                    lsd_score = compute_lsd(ref_file, quant_file)\n",
    "                    results.append({\n",
    "                        \"quantization_level\": quantization_level.name,\n",
    "                        \"file\": quant_file.name,\n",
    "                        \"FAD\": fad_score,\n",
    "                        \"MCD\": mcd_score,\n",
    "                        \"LSD\": lsd_score\n",
    "                    })\n",
    "                    print(f\"File: {quant_file.name}, MCD: {mcd_score}, LSD: {lsd_score}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenL3 model\n",
    "embedding_model = load_audio_embedding_model(content_type=\"music\", input_repr=\"mel256\")\n",
    "\n",
    "# Example directories\n",
    "reference_audio_dir = \"reference_audio\"  # path to high-quality baseline audio\n",
    "quantized_audio_dir = \"quantized_audio\"  # path to quantized audio organized by level\n",
    "\n",
    "# Run the test\n",
    "results = test_audio_quality(reference_audio_dir, quantized_audio_dir, embedding_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import gin\n",
    "from functools import partial\n",
    "import copy\n",
    "from gamadhani.utils.utils import download_models, download_data\n",
    "from gamadhani.utils.generate_utils import load_audio_fns\n",
    "from gamadhani.utils.utils import get_device\n",
    "import gamadhani.utils.pitch_to_audio_utils as p2a\n",
    "\n",
    "\n",
    "# Load Model\n",
    "_, _, audio_path, audio_qt = download_models('kmaneeshad/GaMaDHaNi', pitch_model_type=\"diffusion\")\n",
    "base_model, audio_qt, audio_seq_len, invert_audio_fn = load_audio_fns(audio_path=audio_path, qt_path=audio_qt, config_path='../configs/pitch_to_audio_config.gin')\n",
    "\n",
    "# Parse pitch config\n",
    "gin.parse_config_file('../configs/diffusion_pitch_config.gin')\n",
    "Task_ = gin.get_configurable('src.dataset.Task')\n",
    "task_obj = Task_()\n",
    "pitch_task_fn = partial(task_obj.read_)\n",
    "invert_pitch_task_fn = partial(task_obj.invert_)\n",
    "\n",
    "\n",
    "# Prepare Test Set\n",
    "test_set = np.load(download_data('kmaneeshad/GaMaDHaNi-db'), allow_pickle=True)['concatenated_array']\n",
    "n_test = test_set.shape[0]\n",
    "test_pitches = np.array([test_set[i,0][:2400] for i in range(n_test)])\n",
    "test_audios = np.array([test_set[i,1] for i in range(n_test)])\n",
    "test_audios_resampled_truncated = torch.stack([torchaudio.functional.resample(torch.tensor(test_audios[i]), orig_freq=44100, new_freq=16000) for i in range(n_test)])[...,:191744]\n",
    "test_singer_ids = np.array([test_set[i,4] for i in range(n_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f0_input(pitch_val):\n",
    "    processed_pitch_val = pitch_task_fn(**{\"inputs\": {\"pitch\": {\"data\": pitch_val}}})['sampled_sequence']\n",
    "    processed_pitch_val = torch.Tensor(processed_pitch_val).reshape(1, 1, processed_pitch_val.shape[0])\n",
    "    interpolated_pitch = p2a.interpolate_pitch(pitch=processed_pitch_val, audio_seq_len=audio_seq_len)    # interpolate pitch values\n",
    "    interpolated_pitch = torch.nan_to_num(interpolated_pitch, nan=196)\n",
    "    interpolated_pitch = interpolated_pitch.squeeze(1)\n",
    "    f0 = interpolated_pitch.float()\n",
    "    return f0\n",
    "\n",
    "def generate_audio(model, pitch_val, singer_id, num_steps=100, audio_only=True):\n",
    "    model = model.cuda()\n",
    "    f0 = get_f0_input(pitch_val).to(model.device)\n",
    "    singer_tensor = torch.tensor(np.repeat([singer_id], repeats=f0.shape[0])).to(model.device)\n",
    "\n",
    "    if audio_only:\n",
    "        out_spec, _, _, _ =  model.sample_cfg(f0.shape[0], f0=f0, num_steps=num_steps, singer=singer_tensor, strength=3, invert_audio_fn=invert_audio_fn)\n",
    "        audio = invert_audio_fn(out_spec)\n",
    "        return audio\n",
    "    else:\n",
    "        out_spec, out_pitch, singer_id, all_activations = model.sample_cfg(f0.shape[0], f0=f0, num_steps=num_steps, singer=singer_tensor, strength=3, invert_audio_fn=invert_audio_fn, log_interim_samples=True, log_interim_forward_activations=True)\n",
    "        audio = invert_audio_fn(out_spec)\n",
    "        return out_spec, out_pitch, singer_id, all_activations, audio\n",
    "  \n",
    "def generate_all_audios(model, num_steps):\n",
    "  generated_audios = []\n",
    "  for i in range(n_test):\n",
    "     generated_audios.append(generate_audio(model, test_pitches[i], test_singer_ids[i], num_steps=num_steps))\n",
    "  return torch.cat(generated_audios, dim=0)\n",
    "    \n",
    "\n",
    "def quantize_per_tensor(x, bits=4, min_val=None, max_val=None):\n",
    "    if min_val is None:\n",
    "        min_val = x.min()\n",
    "    if max_val is None:\n",
    "        max_val = x.max()\n",
    "    targets = torch.linspace(min_val, max_val, 2**bits).to(x.device)\n",
    "    differences = torch.abs(x.unsqueeze(-1) - targets)\n",
    "    nearest_indices = torch.argmin(differences, dim=-1)\n",
    "    rounded_values = targets[nearest_indices]\n",
    "    return rounded_values\n",
    "\n",
    "\n",
    "def get_input_ranges(in_dim, layer_type):\n",
    "  if layer_type == \"conv\":\n",
    "    if in_dim == 512:\n",
    "      return [range(0, 256),\n",
    "              range(256, 384),\n",
    "              range(384, 512)]\n",
    "    elif in_dim == 768:\n",
    "      return [range(0, 512),\n",
    "              range(512, 640),\n",
    "              range(640, 768)]\n",
    "    elif in_dim == 896:\n",
    "      return [range(0, 640),\n",
    "              range(640, 768),\n",
    "              range(768, 896)]\n",
    "    else:\n",
    "      return [range(0, in_dim)]\n",
    "    \n",
    "  elif layer_type == \"convtranspose\":\n",
    "    if in_dim == 2432:\n",
    "      return [range(0, 1024),\n",
    "              range(1024, 1152),\n",
    "              range(1152, 1280),\n",
    "              range(1280, 2304),\n",
    "              range(2304, 2432)]\n",
    "    elif in_dim == 1664:\n",
    "      return [range(0, 640),\n",
    "              range(640, 768),\n",
    "              range(768, 896),\n",
    "              range(896, 1536),\n",
    "              range(1536, 1664)]\n",
    "    elif in_dim == 1408:\n",
    "      return [range(0, 512),\n",
    "              range(512, 640),\n",
    "              range(640, 768),\n",
    "              range(768, 1280),\n",
    "              range(1280, 1408)]\n",
    "    elif in_dim == 640:\n",
    "      return [range(0, 256),\n",
    "              range(256, 512),\n",
    "              range(512, 640)]\n",
    "    else:\n",
    "      return [range(0, in_dim)] \n",
    "\n",
    "\n",
    "\n",
    "def quantize_model(net, bits, mode='global'):\n",
    "  quantized_net = copy.deepcopy(net)\n",
    "  for name, module in quantized_net.named_modules():\n",
    "    if isinstance(module, (torch.nn.Conv1d, torch.nn.Linear, torch.nn.ConvTranspose1d)):\n",
    "      if mode == 'global':\n",
    "        module.weight.data = quantize_per_tensor(module.weight.data, bits=bits)\n",
    "      \n",
    "      elif mode == 'per_channel':        \n",
    "        if isinstance(module, (torch.nn.Conv1d)):\n",
    "          for i in range(module.weight.data.shape[0]):\n",
    "            module.weight.data[i] = quantize_per_tensor(module.weight.data[i], bits=bits)\n",
    "        elif isinstance(module, (torch.nn.ConvTranspose1d)):\n",
    "          for i in range(module.weight.data.shape[1]):\n",
    "            module.weight.data[:,i] = quantize_per_tensor(module.weight.data[:,i], bits=bits)\n",
    "        else:\n",
    "          module.weight.data = quantize_per_tensor(module.weight.data, bits=bits)   \n",
    "\n",
    "      elif mode == 'grouped_input_channels':       \n",
    "        if isinstance(module, (torch.nn.Conv1d)):\n",
    "          \n",
    "          num_output_channels = module.weight.data.shape[0]\n",
    "          num_input_channels = module.weight.data.shape[1]\n",
    "          input_ranges = get_input_ranges(num_input_channels, \"conv\")\n",
    "          \n",
    "          for i in range(num_output_channels):\n",
    "            for input_range in input_ranges:\n",
    "              print(input_range)\n",
    "              module.weight.data[i, input_range] = quantize_per_tensor(module.weight.data[i, input_range], bits=bits)\n",
    "            \n",
    "        elif isinstance(module, (torch.nn.ConvTranspose1d)):\n",
    "          num_output_channels = module.weight.data.shape[1]\n",
    "          num_input_channels = module.weight.data.shape[0]\n",
    "          input_ranges = get_input_ranges(num_input_channels, \"convtranspose\")\n",
    "          \n",
    "          for i in range(num_output_channels):\n",
    "            for input_range in input_ranges:\n",
    "              #print(input_range)\n",
    "              module.weight.data[input_range,i] = quantize_per_tensor(module.weight.data[input_range,i], bits=bits)\n",
    "        else:\n",
    "          module.weight.data = quantize_per_tensor(module.weight.data, bits=bits)   \n",
    "      else:\n",
    "         raise ValueError\n",
    "\n",
    "  if hasattr(module, 'bias') and module.bias is not None and mode == 'per_channel':\n",
    "      module.bias.data = quantize_per_tensor(module.bias.data, bits=bits)\n",
    "  return quantized_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Generation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_generated = generate_all_audios(base_model, 100)\n",
    "torch.save(torch.nan_to_num(base_generated,0), \"examples/base_model.pt\")\n",
    "torch.save(test_audios_resampled_truncated, \"examples/ground_truth.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"per_chan_8_bit\"\n",
    "q_model = quantize_model(base_model, bits=8, mode='per_channel')\n",
    "generated = generate_all_audios(q_model, 100)\n",
    "torch.save(torch.nan_to_num(generated,0), f\"examples/{name}.pt\")\n",
    "\n",
    "for i in range(16):\n",
    "    display(ipd.Audio(generated[i].cpu(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"per_group_8_bit\"\n",
    "q_model = quantize_model(base_model, bits=8, mode='grouped_input_channels')\n",
    "generated = generate_all_audios(q_model, 100)\n",
    "torch.save(torch.nan_to_num(generated,0), f\"examples/{name}.pt\")\n",
    "\n",
    "for i in range(16):\n",
    "    display(ipd.Audio(generated[i].cpu(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"per_tensor_8_bit\"\n",
    "q_model = quantize_model(base_model, bits=8, mode='global')\n",
    "generated = generate_all_audios(q_model, 10)\n",
    "#torch.save(torch.nan_to_num(generated,0), f\"examples/{name}.pt\")\n",
    "\n",
    "# for i in range(16):\n",
    "#     display(ipd.Audio(generated[i].cpu(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"per_tensor_4_bit\"\n",
    "q_model = quantize_model(base_model, bits=4, mode='global')\n",
    "generated = generate_all_audios(q_model, 3)\n",
    "torch.save(torch.nan_to_num(generated,0), f\"examples/{name}.pt\")\n",
    "\n",
    "for i in range(16):\n",
    "    display(ipd.Audio(generated[i].cpu(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"per_chan_4_bit\"\n",
    "q_model = quantize_model(base_model, bits=4, mode='per_channel')\n",
    "generated = generate_all_audios(q_model, 4)\n",
    "torch.save(torch.nan_to_num(generated,0), f\"examples/{name}.pt\")\n",
    "\n",
    "for i in range(16):\n",
    "    display(ipd.Audio(generated[i].cpu(), rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"per_group_4_bit\"\n",
    "q_model = quantize_model(base_model, bits=4, mode='grouped_input_channels')\n",
    "generated = generate_all_audios(q_model, 4)\n",
    "torch.save(torch.nan_to_num(generated,0), f\"examples/{name}.pt\")\n",
    "\n",
    "for i in range(16):\n",
    "    display(ipd.Audio(generated[i].cpu(), rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_audio(base_model, pitch_val=test_pitches[0], singer_id=test_singer_ids[0], num_steps=100, audio_only=False)\n",
    "activations = out[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "mags = torch.abs(activations[0.50]['upsample_layer_input_0'])[0].cpu().T\n",
    "\n",
    "def plot_activations(mags, title=\"\", figsize=(12, 12), dpi=300, mode='weight'):\n",
    "    \n",
    "    num_timeframes = mags.shape[0]\n",
    "    num_channels = mags.shape[1]\n",
    "\n",
    "    ranges = get_input_ranges(num_channels, \"convtranspose\")\n",
    "    \n",
    "    # Create figure with specific layout parameters\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    plt.subplots_adjust(top=0.9, bottom=0.15, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Adjust subplot parameters to give more space\n",
    "    #plt.subplots_adjust(top=0.9, bottom=0.15)\n",
    "    \n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    x, y = np.meshgrid(\n",
    "        np.arange(num_channels),\n",
    "        np.arange(num_timeframes),\n",
    "    )\n",
    "\n",
    "    # Custom legend handles\n",
    "    legend_handles = []\n",
    "    print(len(ranges))\n",
    "    names = ['Prev. Activation','Time-Step Condition', 'Singer ID Condition', 'Skip Connection', 'Pitch Condition']\n",
    "\n",
    "    # Loop through ranges and plot each channel group with a different color\n",
    "    for i, curr_range in enumerate(ranges):\n",
    "        z = mags[:, curr_range]\n",
    "        x_range, y_range = np.meshgrid(\n",
    "            np.arange(curr_range.start, curr_range.stop),\n",
    "            np.arange(num_timeframes),\n",
    "        )\n",
    "        # Use a perceptually uniform colormap for better distinction\n",
    "        color_map = plt.cm.get_cmap(\"viridis\", len(ranges))\n",
    "        color = color_map(i)\n",
    "\n",
    "        ax.plot_surface(\n",
    "            x_range, y_range, z,\n",
    "            color=color,\n",
    "            alpha=0.7,  # Slightly reduced alpha for depth perception\n",
    "            edgecolor=\"none\",\n",
    "            rstride=1,\n",
    "            cstride=1,\n",
    "        )\n",
    "\n",
    "        # Add a legend entry for the current range\n",
    "        legend_handles.append(mpatches.Patch(color=color, label=names[i]))\n",
    "\n",
    "    # Styling improvements\n",
    "    \n",
    "    if mode=='weight':\n",
    "        ax.set_xlabel(\"Input Channel\", fontweight='bold', fontsize=24)\n",
    "        ax.set_ylabel(\"Output Channel\", fontweight='bold', fontsize=24)\n",
    "        ax.set_zlabel(\"Avg. Weight Magnitude\", fontweight='bold', fontsize=24)\n",
    "    else:\n",
    "        ax.set_xlabel(\"Channel\", fontweight='bold', fontsize=24)\n",
    "        ax.set_ylabel(\"Time Frame\", fontweight='bold', fontsize=24)\n",
    "        ax.set_zlabel(\"Activation Magnitude\", fontweight='bold', fontsize=24)\n",
    "    ax.view_init(elev=20, azim=260)\n",
    "    \n",
    "\n",
    "    # Add title if provided, with reduced spacing\n",
    "    if title:\n",
    "        fig.suptitle(title, fontweight='bold', fontsize=30, y=0.87) \n",
    "\n",
    "    # Add a legend closer to the plot\n",
    "    plt.legend(\n",
    "        handles=legend_handles, \n",
    "        loc='upper center', \n",
    "        bbox_to_anchor=(0.5, 0.05),  # Moved closer to the plot\n",
    "        frameon=False, \n",
    "        ncol=2,\n",
    "        fontsize=24\n",
    "    )\n",
    "\n",
    "    # Reduce whitespace and adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save as transparent SVG\n",
    "    plt.savefig(f'{title}_activations.png', \n",
    "                transparent=True, \n",
    "                bbox_inches='tight', \n",
    "                pad_inches=0)\n",
    "\n",
    "    # Optional: show the plot   \n",
    "    plt.show()\n",
    "\n",
    "# Example function call\n",
    "plot_activations(mags, 'Inputs to First Upsampling Layer', mode='asdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.mean(torch.abs(base_model.upsample_layers[0].process_layer.convs[0].conv.weight.data.cpu()), dim=-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(weights, \"First ConvTranpose\", mode='weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box and Whiskey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_audio(base_model, pitch_val=test_pitches[0], singer_id=test_singer_ids[0], num_steps=100, audio_only=False)\n",
    "\n",
    "plt.figure(figsize=(14, 8), dpi=300)\n",
    "plt.boxplot([interim_activations[i].cpu().numpy().flatten() for i in list(interim_activations.keys())[::5]], \n",
    "            vert=True, \n",
    "            tick_labels=[int(k*100) for k in list(interim_activations.keys())[::5]],\n",
    "            flierprops=dict(alpha=0.1, marker='o'))\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Activations Over Diffusion Steps\", fontsize=30)\n",
    "plt.xlabel(\"Diffusion Step\", fontsize=24)\n",
    "plt.ylabel(\"Activation Values\", fontsize=24)\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)  # For major ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save with transparency\n",
    "plt.savefig('activations_boxplot_nb.svg', dpi=300, transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divergence of Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_dict = {}\n",
    "\n",
    "for num_bits in [8,4]:\n",
    "    for quantization_method in ['global', 'per_channel', 'grouped_input_channels']:\n",
    "        baseline_name = quantization_method+\"_\"+str(num_bits)\n",
    "        q_model = quantize_model(base_model, bits=num_bits, mode=quantization_method)\n",
    "        out = generate_audio(q_model, pitch_val=test_pitches[0], singer_id=test_singer_ids[0], num_steps=100, audio_only=False)\n",
    "        samples, _, singers, (interim_activations, _, all_activation), _ = out\n",
    "\n",
    "        relevant_keys = ['downsample_layer_0', 'downsample_layer_1', 'downsample_layer_2', 'upsample_layer_0', 'upsample_layer_1', 'upsample_layer_2']\n",
    "        activation_list = []\n",
    "\n",
    "        for t in np.linspace(0.0, 0.99, 100):\n",
    "            for relkey in relevant_keys:\n",
    "                activation_list.append(torch.mean(torch.abs(all_activation[t][relkey])).cpu())\n",
    "        \n",
    "        trajectories_dict[baseline_name] = torch.stack(activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate_audio(base_model, pitch_val=test_pitches[0], singer_id=test_singer_ids[0], num_steps=100, audio_only=False)\n",
    "samples, _, singers, (interim_activations, _, all_activation), _ = out\n",
    "\n",
    "relevant_keys = ['downsample_layer_0', 'downsample_layer_1', 'downsample_layer_2', 'upsample_layer_0', 'upsample_layer_1', 'upsample_layer_2']\n",
    "activation_list = []\n",
    "\n",
    "for t in np.linspace(0.0, 0.99, 100):\n",
    "    for relkey in relevant_keys:\n",
    "        activation_list.append(torch.mean(torch.abs(all_activation[t][relkey])).cpu())\n",
    "\n",
    "trajectories_dict['base_model'] = torch.stack(activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5), dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "for name in ['base_model', 'global_8','grouped_input_channels_8']:\n",
    "    \n",
    "    if name == 'base_model':\n",
    "        realname = \"Base\"\n",
    "    elif name == 'global_8':\n",
    "        realname = \"Per-Tensor\"\n",
    "    else:\n",
    "        realname = \"Ours\"\n",
    "    traj = trajectories_dict[name]\n",
    "    plt.plot(np.linspace(0, 99, 100), traj[::6], label = realname)\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')  \n",
    "plt.ylim(0,5)\n",
    "plt.title(\"Activations Over Time\", fontsize=30)\n",
    "plt.xlabel(\"Diffusion Step\", fontsize=24)\n",
    "plt.ylabel(\"Activation Mag.\", fontsize=24)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)  # For major ticks\n",
    "\n",
    "\n",
    "# Save with transparency\n",
    "plt.savefig('activations_over_time_nb.svg', dpi=300, transparent=True, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

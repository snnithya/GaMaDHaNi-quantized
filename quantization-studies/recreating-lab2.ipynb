{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_activation = {}\n",
    "output_activation = {}\n",
    "def add_range_recoder_hook(model):\n",
    "    import functools\n",
    "    def _record_range(self, x, y, module_name):\n",
    "        x = x[0]\n",
    "        input_activation[module_name] = x.detach()\n",
    "        output_activation[module_name] = y.detach()\n",
    "\n",
    "    all_hooks = []\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear, nn.ReLU)):\n",
    "            all_hooks.append(m.register_forward_hook(\n",
    "                functools.partial(_record_range, module_name=name)))\n",
    "    return all_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantized_range(bitwidth):\n",
    "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
    "    quantized_min = -(1 << (bitwidth - 1))\n",
    "    return quantized_min, quantized_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor\n",
    "    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [float] scale\n",
    "        [int] zero_point\n",
    "    \"\"\"\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    fp_max = fp_tensor.max().item()\n",
    "    fp_min = fp_tensor.min().item()\n",
    "\n",
    "    scale = (fp_max - fp_min)/(quantized_max - quantized_min)\n",
    "    zero_point = int(round(-2**(bitwidth-1)) - fp_min/scale)\n",
    "\n",
    "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
    "    if zero_point < quantized_min:\n",
    "        zero_point = quantized_min\n",
    "    elif zero_point > quantized_max:\n",
    "        zero_point = quantized_max\n",
    "    else: # convert from float to int using round()\n",
    "        zero_point = round(zero_point)\n",
    "    return scale, int(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_scale_for_weight(weight, bitwidth):\n",
    "    \"\"\"\n",
    "    get quantization scale for single tensor of weight\n",
    "    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [integer] quantization bit width\n",
    "    :return:\n",
    "        [floating scalar] scale\n",
    "    \"\"\"\n",
    "    # we just assume values in weight are symmetric\n",
    "    # we also always make zero_point 0 for weight\n",
    "    fp_max = max(weight.abs().max().item(), 5e-7)\n",
    "    _, quantized_max = get_quantized_range(bitwidth)\n",
    "    return fp_max / quantized_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    linear quantization for single fp_tensor\n",
    "    from\n",
    "        fp_tensor = (quantized_tensor - zero_point) * scale\n",
    "    we have,\n",
    "        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n",
    "    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n",
    "    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n",
    "    :return:\n",
    "        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n",
    "    \"\"\"\n",
    "    assert(fp_tensor.dtype == torch.float)\n",
    "    assert(isinstance(scale, float) or\n",
    "        (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n",
    "    assert(isinstance(zero_point, int) or\n",
    "        (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n",
    "\n",
    "    scaled_tensor = fp_tensor / scale\n",
    "    rounded_tensor = torch.round_(scaled_tensor)\n",
    "    rounded_tensor = rounded_tensor.to(dtype)\n",
    "    shifted_tensor = rounded_tensor + zero_point\n",
    "\n",
    "    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n",
    "    quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
    "    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n",
    "    return quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantize_weight_per_channel(tensor, bitwidth):\n",
    "    \"\"\"\n",
    "    linear quantization for weight tensor\n",
    "        using different scales and zero_points for different output channels\n",
    "    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n",
    "    :param bitwidth: [int] quantization bit width\n",
    "    :return:\n",
    "        [torch.(cuda.)Tensor] quantized tensor\n",
    "        [torch.(cuda.)Tensor] scale tensor\n",
    "        [int] zero point (which is always 0)\n",
    "    \"\"\"\n",
    "    dim_output_channels = 0\n",
    "    num_output_channels = tensor.shape[dim_output_channels]\n",
    "    scale = torch.zeros(num_output_channels, device=tensor.device)\n",
    "    for oc in range(num_output_channels):\n",
    "        _subtensor = tensor.select(dim_output_channels, oc)\n",
    "        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n",
    "        scale[oc] = _scale\n",
    "    scale_shape = [1] * tensor.dim()\n",
    "    scale_shape[dim_output_channels] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n",
    "    return quantized_tensor, scale, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_conv1d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n",
    "             input_zero_point, output_zero_point,\n",
    "             input_scale, weight_scale, output_scale,\n",
    "             stride, padding, dilation, groups):\n",
    "    \"\"\"\n",
    "    quantized 1d convolution\n",
    "    :param input: [torch.CharTensor] quantized input (torch.int8)\n",
    "    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n",
    "    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n",
    "    :param feature_bitwidth: [int] quantization bit width of input and output\n",
    "    :param weight_bitwidth: [int] quantization bit width of weight\n",
    "    :param input_zero_point: [int] input zero point\n",
    "    :param output_zero_point: [int] output zero point\n",
    "    :param input_scale: [float] input feature scale\n",
    "    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n",
    "    :param output_scale: [float] output feature scale\n",
    "    :return:\n",
    "        [torch.(cuda.)CharTensor] quantized output feature\n",
    "    \"\"\"\n",
    "    assert(len(padding) == 4)\n",
    "    assert(input.dtype == torch.int8)\n",
    "    assert(weight.dtype == input.dtype)\n",
    "    assert(bias is None or bias.dtype == torch.int32)\n",
    "    assert(isinstance(input_zero_point, int))\n",
    "    assert(isinstance(output_zero_point, int))\n",
    "    assert(isinstance(input_scale, float))\n",
    "    assert(isinstance(output_scale, float))\n",
    "    assert(weight_scale.dtype == torch.float)\n",
    "\n",
    "    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n",
    "    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n",
    "    if 'cpu' in input.device.type:\n",
    "        # use 32-b MAC for simplicity\n",
    "        output = torch.nn.functional.conv1d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n",
    "    else:\n",
    "        # current version pytorch does not yet support integer-based conv2d() on GPUs\n",
    "        output = torch.nn.functional.conv1d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n",
    "        output = output.round().to(torch.int32)\n",
    "    if bias is not None:\n",
    "        output = output + bias.view(1, -1, 1)\n",
    "\n",
    "    output = output.float()\n",
    "    output = output * (input_scale * weight_scale.view(1, -1, 1)/output_scale)\n",
    "    output = output + output_zero_point\n",
    "\n",
    "    # Make sure all value lies in the bitwidth-bit range\n",
    "    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedConv1d(nn.Module):\n",
    "    def __init__(self, weight, bias,\n",
    "                input_zero_point, output_zero_point,\n",
    "                input_scale, weight_scale, output_scale,\n",
    "                stride, padding, dilation, groups,\n",
    "                feature_bitwidth=8, weight_bitwidth=8):\n",
    "        super().__init__()\n",
    "        # current version Pytorch does not support IntTensor as nn.Parameter\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('bias', bias)\n",
    "\n",
    "        self.input_zero_point = input_zero_point\n",
    "        self.output_zero_point = output_zero_point\n",
    "\n",
    "        self.input_scale = input_scale\n",
    "        self.register_buffer('weight_scale', weight_scale)\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = (padding[1], padding[1], padding[0], padding[0])\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "\n",
    "        self.feature_bitwidth = feature_bitwidth\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return quantized_conv1d(\n",
    "            x, self.weight, self.bias,\n",
    "            self.feature_bitwidth, self.weight_bitwidth,\n",
    "            self.input_zero_point, self.output_zero_point,\n",
    "            self.input_scale, self.weight_scale, self.output_scale,\n",
    "            self.stride, self.padding, self.dilation, self.groups\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
